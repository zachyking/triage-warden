"""End-to-end integration tests for malware/EDR triage.

Tests the complete triage pipeline for malware-related alerts,
verifying correct verdicts, confidence levels, MITRE techniques,
and recommended actions including host isolation.
"""

from __future__ import annotations

import json
import sys
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any
from unittest.mock import MagicMock

import pytest

# =============================================================================
# Mock module setup (must be before imports from tw_ai)
# =============================================================================


@dataclass
class MockToolDefinition:
    """Mock ToolDefinition for testing."""
    name: str
    description: str
    parameters: dict


class _MockLLMBase:
    """Mock tw_ai.llm.base module."""
    ToolDefinition = MockToolDefinition

    class Role:
        SYSTEM = "system"
        USER = "user"
        ASSISTANT = "assistant"
        TOOL = "tool"

    @dataclass
    class Message:
        role: str
        content: str
        name: str | None = None
        tool_call_id: str | None = None

        @classmethod
        def system(cls, content: str) -> "Message":
            return cls(role="system", content=content)

        @classmethod
        def user(cls, content: str) -> "Message":
            return cls(role="user", content=content)

        @classmethod
        def assistant(cls, content: str) -> "Message":
            return cls(role="assistant", content=content)

        @classmethod
        def tool_result(cls, content: str, tool_call_id: str) -> "Message":
            return cls(role="tool", content=content, tool_call_id=tool_call_id)

    @dataclass
    class ToolCall:
        id: str
        name: str
        arguments: dict

    @dataclass
    class LLMResponse:
        content: str | None
        tool_calls: list = None
        finish_reason: str = "stop"
        usage: dict = None
        model: str = ""
        raw_response: Any = None

        def __post_init__(self):
            if self.tool_calls is None:
                self.tool_calls = []
            if self.usage is None:
                self.usage = {}

        @property
        def has_tool_calls(self) -> bool:
            return len(self.tool_calls) > 0

    class LLMProvider:
        @property
        def name(self) -> str:
            return "mock"

        async def complete(self, messages, tools=None, temperature=0.1, max_tokens=4096):
            pass

        async def health_check(self) -> bool:
            return True


sys.modules["tw_ai.llm.base"] = _MockLLMBase
sys.modules["tw_ai.llm"] = MagicMock()


# Mock models module
class MockTriageAnalysis:
    def __init__(self, **kwargs):
        self.verdict = kwargs.get("verdict", "suspicious")
        self.confidence = kwargs.get("confidence", 75)
        self.severity = kwargs.get("severity", "medium")
        self.summary = kwargs.get("summary", "Test summary")
        self.indicators = kwargs.get("indicators", [])
        self.mitre_techniques = kwargs.get("mitre_techniques", [])
        self.recommended_actions = kwargs.get("recommended_actions", [])
        self.reasoning = kwargs.get("reasoning", "Test reasoning")

    def model_dump(self):
        return {
            "verdict": self.verdict,
            "confidence": self.confidence,
            "severity": self.severity,
            "summary": self.summary,
            "indicators": self.indicators,
            "mitre_techniques": self.mitre_techniques,
            "recommended_actions": self.recommended_actions,
            "reasoning": self.reasoning,
        }


class _MockModels:
    TriageAnalysis = MockTriageAnalysis


sys.modules["tw_ai.agents.models"] = _MockModels


# Mock output_parser module
class MockParseError(Exception):
    pass


def mock_parse_triage_analysis(text: str):
    """Mock parser that extracts JSON from text."""
    import re

    json_match = re.search(r"```json\s*([\s\S]*?)\s*```", text)
    if json_match:
        json_str = json_match.group(1)
    else:
        json_match = re.search(r"\{[\s\S]*\}", text)
        if json_match:
            json_str = json_match.group(0)
        else:
            raise MockParseError("No JSON found in response")

    try:
        data = json.loads(json_str)
        return MockTriageAnalysis(**data)
    except json.JSONDecodeError as e:
        raise MockParseError(f"Invalid JSON: {e}")


class _MockOutputParser:
    ParseError = MockParseError
    parse_triage_analysis = staticmethod(mock_parse_triage_analysis)


sys.modules["tw_ai.agents.output_parser"] = _MockOutputParser


# Mock tools module
@dataclass
class MockTool:
    name: str
    description: str
    parameters: dict
    handler: Any
    requires_confirmation: bool = False

    def to_definition(self):
        return MockToolDefinition(
            name=self.name,
            description=self.description,
            parameters=self.parameters,
        )


class MockToolRegistry:
    def __init__(self):
        self._tools: dict[str, MockTool] = {}

    def register(self, tool: MockTool):
        self._tools[tool.name] = tool

    def get(self, name: str):
        return self._tools.get(name)

    def list_tools(self):
        return list(self._tools.keys())

    def get_tool_definitions(self):
        return [tool.to_definition() for tool in self._tools.values()]

    async def execute(self, name: str, arguments: dict):
        tool = self._tools.get(name)
        if not tool:
            raise ValueError(f"Tool not found: {name}")
        return await tool.handler(**arguments)


class _MockTools:
    Tool = MockTool
    ToolRegistry = MockToolRegistry


sys.modules["tw_ai.agents.tools"] = _MockTools


# Import the actual react module
import importlib.util

_base_path = Path(__file__).parent.parent.parent / "tw_ai" / "agents"


def _load_module(name: str, file_path: Path):
    """Load a module directly from file path."""
    spec = importlib.util.spec_from_file_location(name, file_path)
    module = importlib.util.module_from_spec(spec)
    sys.modules[name] = module
    spec.loader.exec_module(module)
    return module


_react = _load_module("tw_ai.agents.react", _base_path / "react.py")
ReActAgent = _react.ReActAgent
AgentResult = _react.AgentResult
TriageRequest = _react.TriageRequest
StepType = _react.StepType


# =============================================================================
# Test Constants
# =============================================================================

# EICAR test file MD5 hash - used as known malicious indicator
EICAR_HASH = "44d88612fea8a8f36de82e1278abb02f"


# =============================================================================
# Test Helper Functions
# =============================================================================


def create_mock_llm_for_malware(response_json: str):
    """Create a mock LLM that returns the given response."""
    from unittest.mock import AsyncMock

    llm = MagicMock()
    llm.name = "mock-llm"
    llm.complete = AsyncMock(return_value=_MockLLMBase.LLMResponse(
        content=f"```json\n{response_json}\n```",
        tool_calls=[],
        usage={"total_tokens": 600},
    ))
    return llm


def get_eicar_malware_response() -> str:
    """Get a malicious EICAR detection analysis response."""
    return json.dumps({
        "verdict": "true_positive",
        "confidence": 95,
        "severity": "critical",
        "summary": "Confirmed malware execution detected. EICAR test file hash identified, "
                   "encoded PowerShell command execution indicates malicious payload delivery.",
        "indicators": [
            {"type": "hash", "value": EICAR_HASH, "verdict": "malicious - EICAR test file"},
            {"type": "process", "value": "powershell.exe -enc", "verdict": "malicious - encoded command execution"},
        ],
        "mitre_techniques": [
            {
                "id": "T1059.001",
                "name": "PowerShell",
                "tactic": "Execution",
                "relevance": "Encoded PowerShell used for payload execution",
            },
            {
                "id": "T1027",
                "name": "Obfuscated Files or Information",
                "tactic": "Defense Evasion",
                "relevance": "Base64 encoded command to evade detection",
            },
        ],
        "recommended_actions": [
            {
                "action": "Isolate host immediately",
                "priority": "immediate",
                "reason": "Contain active malware infection",
            },
            {
                "action": "Block file hash at endpoint",
                "priority": "immediate",
                "reason": "Prevent execution on other hosts",
            },
            {
                "action": "Acquire memory dump for forensics",
                "priority": "high",
                "reason": "Preserve evidence of malicious activity",
            },
            {
                "action": "Reset user credentials",
                "priority": "high",
                "reason": "Assume credential compromise",
            },
        ],
        "reasoning": "Critical malware detection with high confidence. Evidence: "
                     "(1) File hash matches known malware signature (EICAR test file), "
                     "(2) Encoded PowerShell execution is a classic evasion technique, "
                     "(3) Process spawned from cmd.exe suggests execution chain. "
                     "Immediate containment required to prevent lateral movement.",
    })


def get_suspicious_process_response() -> str:
    """Get a suspicious process behavior (credential dumping) response."""
    return json.dumps({
        "verdict": "true_positive",
        "confidence": 98,
        "severity": "critical",
        "summary": "Credential dumping attack detected. Notepad.exe accessing LSASS with full access rights "
                   "is a strong indicator of Mimikatz or similar credential harvesting tool.",
        "indicators": [
            {"type": "process", "value": "notepad.exe -> lsass.exe", "verdict": "malicious - abnormal access pattern"},
            {"type": "access_mask", "value": "0x1fffff", "verdict": "malicious - PROCESS_ALL_ACCESS"},
        ],
        "mitre_techniques": [
            {
                "id": "T1003.001",
                "name": "LSASS Memory",
                "tactic": "Credential Access",
                "relevance": "Direct credential extraction from LSASS",
            },
            {
                "id": "T1055.012",
                "name": "Process Hollowing",
                "tactic": "Defense Evasion",
                "relevance": "Notepad likely hollowed to host malicious code",
            },
        ],
        "recommended_actions": [
            {
                "action": "Isolate host immediately",
                "priority": "immediate",
                "reason": "Active credential theft in progress",
            },
            {
                "action": "Disable compromised service account",
                "priority": "immediate",
                "reason": "Prevent lateral movement",
            },
            {
                "action": "Force password reset for all accounts on host",
                "priority": "immediate",
                "reason": "LSASS contains cached credentials",
            },
        ],
        "reasoning": "Unambiguous credential dumping attack. Critical indicators: "
                     "(1) Notepad.exe should NEVER access LSASS - this is process hollowing, "
                     "(2) Access mask 0x1fffff is PROCESS_ALL_ACCESS, "
                     "(3) Service account executing this suggests prior compromise.",
    })


def get_benign_process_response() -> str:
    """Get a benign process analysis response (e.g., Windows Defender)."""
    return json.dumps({
        "verdict": "false_positive",
        "confidence": 92,
        "severity": "informational",
        "summary": "False positive - Windows Defender antimalware service legitimately accessing LSASS "
                   "as part of credential guard and memory scanning functionality.",
        "indicators": [
            {"type": "process", "value": "MsMpEng.exe", "verdict": "benign - Windows Defender service"},
        ],
        "mitre_techniques": [],
        "recommended_actions": [
            {
                "action": "Add exclusion for Defender LSASS access",
                "priority": "medium",
                "reason": "Reduce alert fatigue from known good behavior",
            },
        ],
        "reasoning": "This is a false positive caused by legitimate security software. "
                     "Process is MsMpEng.exe (Windows Defender) from official Platform directory.",
    })


# =============================================================================
# Integration Tests - Known Malicious Hash (EICAR)
# =============================================================================


class TestMalwareTriageEicar:
    """Tests for known malicious hash detection (EICAR)."""

    @pytest.mark.asyncio
    async def test_eicar_hash_verdict_is_malicious(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that EICAR hash gets malicious verdict."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        # Add hash lookup tool
        async def mock_lookup_hash(hash: str):
            if hash == EICAR_HASH:
                return {
                    "verdict": "malicious",
                    "score": 95,
                    "malware_families": ["EICAR-Test-File"],
                }
            return {"verdict": "unknown", "score": 0}

        registry.register(MockTool(
            name="lookup_hash",
            description="Look up hash reputation",
            parameters={"type": "object", "properties": {"hash": {"type": "string"}}},
            handler=mock_lookup_hash,
        ))

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert result.analysis is not None
        assert result.analysis.verdict == "true_positive"

    @pytest.mark.asyncio
    async def test_eicar_hash_critical_severity(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that EICAR hash gets critical severity."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert result.analysis.severity == "critical"

    @pytest.mark.asyncio
    async def test_eicar_hash_high_confidence(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that EICAR hash gets high confidence score."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert result.analysis.confidence >= 90

    @pytest.mark.asyncio
    async def test_eicar_identifies_execution_techniques(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that malware analysis identifies execution MITRE techniques."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert len(result.analysis.mitre_techniques) > 0

        # Check for execution-related techniques (T1059.*)
        technique_ids = [t["id"] for t in result.analysis.mitre_techniques]
        has_execution_technique = any(
            tid.startswith("T1059") or tid.startswith("T1027")
            for tid in technique_ids
        )
        assert has_execution_technique, f"Expected execution technique, got: {technique_ids}"

    @pytest.mark.asyncio
    async def test_eicar_recommends_host_isolation(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that malware detection recommends host isolation."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert len(result.analysis.recommended_actions) > 0

        # Check for host isolation action
        action_texts = [a["action"].lower() for a in result.analysis.recommended_actions]
        has_isolation = any(
            "isolate" in text or "contain" in text or "quarantine" in text
            for text in action_texts
        )
        assert has_isolation, f"Expected isolation action for critical malware, got: {action_texts}"

    @pytest.mark.asyncio
    async def test_eicar_includes_hash_indicator(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that malware analysis includes hash indicator."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert len(result.analysis.indicators) > 0

        # Check that hash indicator is present
        indicator_types = [i.get("type", "") for i in result.analysis.indicators]
        assert "hash" in indicator_types, f"Expected hash indicator, got types: {indicator_types}"


# =============================================================================
# Integration Tests - Suspicious Process Behavior
# =============================================================================


class TestMalwareTriageSuspiciousProcess:
    """Tests for suspicious process behavior detection (credential dumping)."""

    @pytest.mark.asyncio
    async def test_lsass_access_verdict_is_malicious(
        self,
        mock_tool_registry,
        suspicious_process_alert,
    ):
        """Test that LSASS access by notepad.exe gets malicious verdict."""
        llm = create_mock_llm_for_malware(get_suspicious_process_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=suspicious_process_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert result.analysis.verdict == "true_positive"

    @pytest.mark.asyncio
    async def test_credential_access_mitre_techniques(
        self,
        mock_tool_registry,
        suspicious_process_alert,
    ):
        """Test that credential access MITRE techniques are identified."""
        llm = create_mock_llm_for_malware(get_suspicious_process_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=suspicious_process_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert len(result.analysis.mitre_techniques) > 0

        # Check for credential access techniques (T1003.*)
        technique_ids = [t["id"] for t in result.analysis.mitre_techniques]
        has_credential_access = any(
            tid.startswith("T1003") or tid.startswith("T1055")
            for tid in technique_ids
        )
        assert has_credential_access, f"Expected credential access technique, got: {technique_ids}"

    @pytest.mark.asyncio
    async def test_credential_dumping_recommends_immediate_action(
        self,
        mock_tool_registry,
        suspicious_process_alert,
    ):
        """Test that credential dumping recommends immediate priority actions."""
        llm = create_mock_llm_for_malware(get_suspicious_process_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=suspicious_process_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert len(result.analysis.recommended_actions) > 0

        # Check for immediate priority actions
        priorities = [a.get("priority", "").lower() for a in result.analysis.recommended_actions]
        has_immediate = "immediate" in priorities
        assert has_immediate, f"Expected immediate priority action, got: {priorities}"

    @pytest.mark.asyncio
    async def test_credential_dumping_recommends_password_reset(
        self,
        mock_tool_registry,
        suspicious_process_alert,
    ):
        """Test that credential dumping recommends credential reset."""
        llm = create_mock_llm_for_malware(get_suspicious_process_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=suspicious_process_alert,
        )

        result = await agent.run(request)

        assert result.success is True

        # Check for password reset or credential-related action
        action_texts = [a["action"].lower() for a in result.analysis.recommended_actions]
        has_credential_action = any(
            "password" in text or "credential" in text or "reset" in text or "disable" in text
            for text in action_texts
        )
        assert has_credential_action, f"Expected credential-related action, got: {action_texts}"


# =============================================================================
# Integration Tests - Edge Cases
# =============================================================================


class TestMalwareTriageEdgeCases:
    """Edge case tests for malware triage."""

    @pytest.mark.asyncio
    async def test_malware_with_unknown_hash(self, mock_tool_registry):
        """Test handling of unknown file hash."""
        response = json.dumps({
            "verdict": "suspicious",
            "confidence": 60,
            "severity": "medium",
            "summary": "Unknown file hash with suspicious behavior patterns.",
            "indicators": [
                {"type": "hash", "value": "unknown123", "verdict": "unknown - not in threat intel"},
            ],
            "mitre_techniques": [
                {
                    "id": "T1059.001",
                    "name": "PowerShell",
                    "tactic": "Execution",
                    "relevance": "Encoded PowerShell execution",
                },
            ],
            "recommended_actions": [
                {
                    "action": "Submit file to sandbox for analysis",
                    "priority": "high",
                    "reason": "Determine if file is malicious",
                },
            ],
            "reasoning": "File hash not in threat intelligence, but behavior is suspicious.",
        })

        llm = create_mock_llm_for_malware(response)
        registry = MockToolRegistry()

        alert = {
            "type": "edr_detection",
            "hostname": "workstation-003",
            "process": "unknown.exe",
            "file_hash": "unknown_hash_999",
        }

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=alert,
        )

        result = await agent.run(request)

        assert result.success is True
        # Unknown hash should be suspicious, not definitively malicious or benign
        assert result.analysis.verdict in ("suspicious", "inconclusive")

    @pytest.mark.asyncio
    async def test_malware_includes_process_tree_context(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that process tree context is considered."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        # Alert includes parent process chain
        alert_with_chain = malware_alert.copy()
        alert_with_chain["process_tree"] = [
            {"name": "explorer.exe", "pid": 1000},
            {"name": "cmd.exe", "pid": 2000},
            {"name": "powershell.exe", "pid": 3000},
        ]

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=alert_with_chain,
        )

        result = await agent.run(request)

        assert result.success is True
        # Should still be malicious with process tree context
        assert result.analysis.verdict == "true_positive"

    @pytest.mark.asyncio
    async def test_malware_multiple_indicators(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that analysis includes multiple relevant indicators."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        # Should have multiple indicators for comprehensive analysis
        assert len(result.analysis.indicators) >= 1

    @pytest.mark.asyncio
    async def test_malware_analysis_has_reasoning(
        self,
        mock_tool_registry,
        malware_alert,
    ):
        """Test that malware analysis includes detailed reasoning."""
        llm = create_mock_llm_for_malware(get_eicar_malware_response())
        registry = MockToolRegistry()

        agent = ReActAgent(llm=llm, tools=registry)

        request = TriageRequest(
            alert_type="edr_detection",
            alert_data=malware_alert,
        )

        result = await agent.run(request)

        assert result.success is True
        assert result.analysis.reasoning is not None
        assert len(result.analysis.reasoning) > 50  # Should have substantial reasoning
